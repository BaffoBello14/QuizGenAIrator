# Load the required libraries
import spacy

# Load the spaCy Italian language model
nlp = spacy.load('it_core_news_lg')

# Read the input files
with open('/content/drive/MyDrive/BPM/text.txt', 'r', encoding='utf-8') as text_file:
    text = text_file.read()

with open('/content/drive/MyDrive/BPM/quiz.txt', 'r', encoding='utf-8') as quiz_file:
    quiz_content = quiz_file.readlines()

# Process the text using NLP
doc_text = nlp(text)

# Define a function to calculate the coherence score between a question and the text
def calculate_coherence_score(question, options):
    # Combine the question and options into a single string
    question_text = question + ' '.join(options)

    # Process the combined text using NLP
    doc_combined = nlp(question_text)

    # Calculate the similarity between the combined text and the text
    coherence_score = doc_combined.similarity(doc_text)

    return coherence_score

# Define a function to calculate the uniqueness score based on the number of unique answer options
def calculate_uniqueness_score(options):
    # Calculate the uniqueness score based on the number of unique answer options
    uniqueness_score = len(set(options)) / len(options)

    return uniqueness_score

# Process each question and calculate the coherence, uniqueness, accuracy, and total scores for each group
group_scores = {}
current_group = None
for line in quiz_content:
    line = line.strip()
    if line.startswith("Group:"):
        current_group = line.split(':')[-1].strip()
        if current_group not in group_scores:
            group_scores[current_group] = []
    elif line.startswith("Correct answer:"):
        correct_answer = line.split(':')[-1].strip()
    elif line.startswith("Revised Bloom's Taxonomy level:") or not line:
        continue
    elif not current_group:
        continue
    elif line.startswith("Question:"):
        if current_group:
            question = line.split(':')[-1].strip()
    else:
        options = line.split(':')[-1].strip().split(';')
        if correct_answer in options:
            accuracy_score = 1.0
        else:
            accuracy_score = 0.0

        coherence_score = calculate_coherence_score(question, options)
        uniqueness_score = calculate_uniqueness_score(options)
        total_score = sum([coherence_score, uniqueness_score, accuracy_score])
        group_scores[current_group].append((question, options, coherence_score, uniqueness_score, accuracy_score, total_score))

# Print the standings of all groups with their question scores
for group, question_scores in group_scores.items():
    print(f"Group: {group}")
    question_scores.sort(key=lambda x: x[-1], reverse=True)
    for i, (question, options, coherence_score, uniqueness_score, accuracy_score, total_score) in enumerate(question_scores):
        print(f"   {i+1}. Question: {question}")
        print(f"      Options: {', '.join(options)}")
        print(f"      Coherence: {coherence_score:.2f}")
        print(f"      Uniqueness: {uniqueness_score:.2f}")
        print(f"      Accuracy: {accuracy_score:.2f}")
        print(f"      Total Score: {total_score:.2f}")
        print()
