1. What is the goal of this project?
A) To build a cleaning algorithm for resumes
B) To develop a classifier for job suggestions
C) To create a database of resumes 
D) To perform NLP preprocessing on job postings 
Correct answer: B

2. What is the purpose of Tokenization?
A) To remove stop words from the text 
B) To substitute misspelled words in the text
C) To break textual data into smaller units called tokens 
D) To group together the inflected forms of a word into its lemma 
Correct answer: C

3. What is the measure of the frequency of a word in a document?
A) Mean
B) Median
C) Term Frequency (TF)
D) Inverse Document Frequency (IDF)
Correct answer: C

4. Which classifier had the highest accuracy in this project?
A) Random Forest Classifier 
B) Naive Bayes Classifier 
C) Decision Tree Classifier 
D) Linear Support Vector classifier
Correct answer: D

5. Which category had the most misclassified resumes?
A) Healthcare
B) Education
C) IT
D) Sales
Correct answer: C